{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DRSNAJ/BERT-LSTM-sentiment-trader/blob/main/BERT_LSTM_news_sentiment_trader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDG86nGvyoP5",
        "outputId": "3a10ae42-7bc2-4290-8bc6-916fbb551dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aCHmhxgRy1BW"
      },
      "outputs": [],
      "source": [
        "import os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as md\n",
        "from datetime import datetime\n",
        "import time\n",
        "import plotly.express as px\n",
        "\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.nn.functional import softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50PM-eDLK_LG"
      },
      "source": [
        "#### Model Training and Loading Configuration\n",
        "\n",
        "This section of the code defines the control flow for handling a machine learning model, specifically deciding whether to train a new model or load a pre-existing one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LszjZ5E6otkp"
      },
      "outputs": [],
      "source": [
        "train_model = False\n",
        "load_model = '/content/drive/MyDrive/Colab Notebooks/Saved Models/log_pert_LSTM-model-2024-05-11_1035.keras'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3JaBTG8K_LG"
      },
      "source": [
        "# Data Loading and Processing for News Tweets\n",
        "\n",
        "This script is designed to load and process tweet data from various news outlets, preparing it for further analysis. Below is a step-by-step breakdown of what each section of the code accomplishes:\n",
        "\n",
        "1. **Initialize File Paths and Data Structures**:\n",
        "    - `dataset_src` specifies the directory containing the tweet data files.\n",
        "    - `news_df` initializes an empty DataFrame to store all tweets from different files.\n",
        "    - `datainfo` is a dictionary to hold summary information about the tweets for each news outlet such as start and end dates and the number of tweets.\n",
        "\n",
        "2. **List of News Outlets**:\n",
        "    - `all_outlets` lists all news outlets for which the tweets are being analyzed.\n",
        "\n",
        "3. **Load and Process Each File**:\n",
        "    - Iterates through each file in the specified dataset source directory.\n",
        "    - Reads each tweet file into a DataFrame and parses the `timestamp` from separate `date` and `time` columns.\n",
        "    - Initializes columns for each news outlet in the DataFrame to mark which tweets belong to which outlet using flags (0 or 1).\n",
        "  \n",
        "4. **Identify and Mark the Data Source**:\n",
        "    - Determines the news outlet by matching the file name and updates the corresponding outlet column to 1.\n",
        "    - Updates the `datainfo` dictionary with the earliest and latest timestamps and the total number of tweets for each outlet.\n",
        "\n",
        "5. **Handle Exceptions**:\n",
        "    - A try-except block to gracefully handle any errors during file processing, such as missing files.\n",
        "\n",
        "6. **Concatenate DataFrames**:\n",
        "    - Appends the data from each file to `news_df`.\n",
        "    - Retains only the necessary columns, focusing on timestamps, outlet flags, and tweet content.\n",
        "\n",
        "7. **Round Timestamps**:\n",
        "    - Rounds off the `timestamp` in `news_df` to the nearest minute to standardize the times for easier analysis.\n",
        "\n",
        "8. **Clean Tweet Content**:\n",
        "    - Uses regular expressions to remove URLs and specific phrases from the tweet text to clean and standardize the data.\n",
        "\n",
        "9. **Summarize and Display Data**:\n",
        "    - Constructs a formatted string to display a summary of the data collected, including the total number of tweets and the date range for tweets from each outlet.\n",
        "    - Finally, prints the summary table to the console.\n",
        "\n",
        "This structured approach efficiently organizes the tweet data for easy access and manipulation in subsequent analyses.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_src = '/content/drive/MyDrive/Colab Notebooks/datasets/news_tweets' # locations of the tweet csv files\n",
        "news_df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "gCy2_-XJXkGu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_src = '/content/drive/MyDrive/Colab Notebooks/datasets/news_tweets' # locations of the tweet csv files\n",
        "news_df = pd.DataFrame()\n",
        "\n",
        "all_outlets = ['bbc', 'cnn', 'eco']\n",
        "\n",
        "datainfo = {\"bbc\":{\"start_data\":None,\"end_data\":None,\"num_tweets\":None},\n",
        "            \"cnn\":{\"start_data\":None,\"end_data\":None,\"num_tweets\":None},\n",
        "            \"eco\":{\"start_data\":None,\"end_data\":None,\"num_tweets\":None}}\n",
        "\n",
        "\n",
        "for fileItem in os.listdir(dataset_src):\n",
        "  data = pd.read_csv(dataset_src + \"/\" + fileItem)\n",
        "  data['timestamp'] = pd.to_datetime((data['date'] + ' ' + data['time']))\n",
        "\n",
        "  for i in all_outlets:\n",
        "    data[i] = 0\n",
        "\n",
        "  try:\n",
        "    outlet = 'unknown'\n",
        "    if (fileItem == 'tweets_bbc.csv'):\n",
        "      outlet = \"bbc\"\n",
        "    elif (fileItem == 'tweets_cnn.csv'):\n",
        "      outlet = \"cnn\"\n",
        "    elif (fileItem == 'tweets_eco.csv'):\n",
        "      outlet = \"eco\"\n",
        "    else:\n",
        "      outlet = 'unknown'\n",
        "\n",
        "    data[outlet] = 1\n",
        "\n",
        "    datainfo[outlet][\"start_data\"] = data[data['timestamp'] == data['timestamp'].min()]['timestamp'].item()\n",
        "    datainfo[outlet][\"end_data\"] = data[data['timestamp'] == data['timestamp'].max()]['timestamp'].item()\n",
        "    datainfo[outlet][\"num_tweets\"] = data.shape[0]\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"File not found: {e}\")\n",
        "\n",
        "\n",
        "  news_df = pd.concat([news_df,data])\n",
        "\n",
        "  print('The total columns of dataset: ' + str(list(news_df.columns)))\n",
        "  news_df = news_df[['timestamp','bbc','cnn','eco','tweet','replies_count', 'retweets_count', 'likes_count']]\n",
        "\n",
        "# news_df['timestamp'] = news_df['timestamp'].round('min') # minute\n",
        "\n",
        "news_df.sort_values(by=['timestamp'])\n",
        "\n",
        "\n",
        "# Regular expression pattern to match URLs\n",
        "tweet_link_format = r'(\\s)http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "\n",
        "# Replace URLs with an empty string\n",
        "news_df['tweet'] = news_df['tweet'].str.replace(tweet_link_format, '', regex=True)\n",
        "news_df['tweet'] = news_df['tweet'].str.replace('. Follow live updates:', '', regex=True)\n",
        "\n",
        "# Printing results\n",
        "total_tweets = 0\n",
        "data_tbl = '\\n==================================DATA SUMMARY===================================\\n|Outlet \\t|Start Date \\t\\t|End Date \\t\\t|Tweets \\t|\\n|---------------|-----------------------|-----------------------|---------------|\\n'\n",
        "for outlet in all_outlets:\n",
        "  data_tbl = data_tbl + '|'+ outlet + '\\t\\t|'+ str(datainfo[outlet][\"start_data\"]) + '\\t|'+ str(datainfo[outlet][\"end_data\"]) + '\\t|'+ str(datainfo[outlet][\"num_tweets\"]) +'\\t\\t|\\n'\n",
        "  total_tweets = total_tweets + datainfo[outlet][\"num_tweets\"]\n",
        "data_tbl = data_tbl +'|---------------|-----------------------|-----------------------|---------------|\\n' + '|\\t\\t|\\t\\t\\t|\\t\\t\\t|' + str(total_tweets) + '\\t\\t|'\n",
        "data_tbl = data_tbl +'\\n=================================================================================\\n'\n",
        "print(data_tbl)\n",
        "\n",
        "news_df.set_index('timestamp', inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "2kWhcLqHcXwW",
        "outputId": "e79077e1-1c6e-4412-9528-828e4b6d86cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2638f638-b59b-4023-a97d-09af5474faeb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2638f638-b59b-4023-a97d-09af5474faeb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2638f638-b59b-4023-a97d-09af5474faeb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2638f638-b59b-4023-a97d-09af5474faeb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_0d60f4cd-3182-4fef-976b-fbbb7fdeb067\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('news_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0d60f4cd-3182-4fef-976b-fbbb7fdeb067 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('news_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "news_df",
              "summary": "{\n  \"name\": \"news_df\",\n  \"rows\": 0,\n  \"fields\": []\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL5yMDV7zCMf",
        "outputId": "400cdb2f-0f0d-428c-d95d-0d9ea6804197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-92fc1d6cf695>:12: DtypeWarning: Columns (22,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(dataset_src + \"/\" + fileItem)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total columns of dataset: ['id', 'conversation_id', 'created_at', 'date', 'time', 'timezone', 'user_id', 'username', 'name', 'place', 'tweet', 'language', 'mentions', 'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags', 'cashtags', 'link', 'retweet', 'quote_url', 'video', 'thumbnail', 'near', 'geo', 'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src', 'trans_dest', 'timestamp', 'bbc', 'cnn', 'eco']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-92fc1d6cf695>:12: DtypeWarning: Columns (22,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(dataset_src + \"/\" + fileItem)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total columns of dataset: ['timestamp', 'bbc', 'cnn', 'eco', 'tweet', 'replies_count', 'retweets_count', 'likes_count', 'id', 'conversation_id', 'created_at', 'date', 'time', 'timezone', 'user_id', 'username', 'name', 'place', 'language', 'mentions', 'urls', 'photos', 'hashtags', 'cashtags', 'link', 'retweet', 'quote_url', 'video', 'thumbnail', 'near', 'geo', 'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src', 'trans_dest']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-92fc1d6cf695>:12: DtypeWarning: Columns (22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(dataset_src + \"/\" + fileItem)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total columns of dataset: ['timestamp', 'bbc', 'cnn', 'eco', 'tweet', 'replies_count', 'retweets_count', 'likes_count', 'id', 'conversation_id', 'created_at', 'date', 'time', 'timezone', 'user_id', 'username', 'name', 'place', 'language', 'mentions', 'urls', 'photos', 'hashtags', 'cashtags', 'link', 'retweet', 'quote_url', 'video', 'thumbnail', 'near', 'geo', 'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src', 'trans_dest']\n",
            "\n",
            "==================================DATA SUMMARY===================================\n",
            "|Outlet \t|Start Date \t\t|End Date \t\t|Tweets \t|\n",
            "|---------------|-----------------------|-----------------------|---------------|\n",
            "|bbc\t\t|2010-01-01 19:40:04\t|2021-07-02 15:28:43\t|34547\t\t|\n",
            "|cnn\t\t|2010-01-01 06:58:23\t|2021-07-05 05:08:12\t|55236\t\t|\n",
            "|eco\t\t|2010-01-01 21:20:14\t|2021-07-05 04:59:39\t|254413\t\t|\n",
            "|---------------|-----------------------|-----------------------|---------------|\n",
            "|\t\t|\t\t\t|\t\t\t|344196\t\t|\n",
            "=================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# dataset_src = '/content/drive/MyDrive/Colab Notebooks/datasets/news_tweets' # locations of the tweet csv files\n",
        "# news_df = pd.DataFrame()\n",
        "\n",
        "# all_outlets = ['bbc', 'cnn', 'eco']\n",
        "\n",
        "# datainfo = {\"bbc\":{\"start_data\":None,\"end_data\":None,\"num_tweets\":None},\n",
        "#             \"cnn\":{\"start_data\":None,\"end_data\":None,\"num_tweets\":None},\n",
        "#             \"eco\":{\"start_data\":None,\"end_data\":None,\"num_tweets\":None}}\n",
        "\n",
        "\n",
        "# for fileItem in os.listdir(dataset_src):\n",
        "#   data = pd.read_csv(dataset_src + \"/\" + fileItem)\n",
        "#   data['timestamp'] = pd.to_datetime((data['date'] + ' ' + data['time']))\n",
        "\n",
        "#   for i in all_outlets:\n",
        "#     data[i] = 0\n",
        "\n",
        "#   try:\n",
        "#     outlet = 'unknown'\n",
        "#     if (fileItem == 'tweets_bbc.csv'):\n",
        "#       outlet = \"bbc\"\n",
        "#     elif (fileItem == 'tweets_cnn.csv'):\n",
        "#       outlet = \"cnn\"\n",
        "#     elif (fileItem == 'tweets_eco.csv'):\n",
        "#       outlet = \"eco\"\n",
        "#     else:\n",
        "#       outlet = 'unknown'\n",
        "\n",
        "#     data[outlet] = 1\n",
        "\n",
        "#     datainfo[outlet][\"start_data\"] = data[data['timestamp'] == data['timestamp'].min()]['timestamp'].item()\n",
        "#     datainfo[outlet][\"end_data\"] = data[data['timestamp'] == data['timestamp'].max()]['timestamp'].item()\n",
        "#     datainfo[outlet][\"num_tweets\"] = data.shape[0]\n",
        "\n",
        "#   except Exception as e:\n",
        "#     print(f\"File not found: {e}\")\n",
        "\n",
        "\n",
        "#   news_df = pd.concat([news_df,data])\n",
        "\n",
        "#   print('The total columns of dataset: ' + str(list(news_df.columns)))\n",
        "#   news_df = news_df[['timestamp','bbc','cnn','eco','tweet','replies_count', 'retweets_count', 'likes_count']]\n",
        "\n",
        "# # news_df['timestamp'] = news_df['timestamp'].round('min') # minute\n",
        "\n",
        "# news_df.sort_values(by=['timestamp'])\n",
        "\n",
        "\n",
        "# # Regular expression pattern to match URLs\n",
        "# tweet_link_format = r'(\\s)http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "\n",
        "# # Replace URLs with an empty string\n",
        "# news_df['tweet'] = news_df['tweet'].str.replace(tweet_link_format, '', regex=True)\n",
        "# news_df['tweet'] = news_df['tweet'].str.replace('. Follow live updates:', '', regex=True)\n",
        "\n",
        "# # Printing results\n",
        "# total_tweets = 0\n",
        "# data_tbl = '\\n==================================DATA SUMMARY===================================\\n|Outlet \\t|Start Date \\t\\t|End Date \\t\\t|Tweets \\t|\\n|---------------|-----------------------|-----------------------|---------------|\\n'\n",
        "# for outlet in all_outlets:\n",
        "#   data_tbl = data_tbl + '|'+ outlet + '\\t\\t|'+ str(datainfo[outlet][\"start_data\"]) + '\\t|'+ str(datainfo[outlet][\"end_data\"]) + '\\t|'+ str(datainfo[outlet][\"num_tweets\"]) +'\\t\\t|\\n'\n",
        "#   total_tweets = total_tweets + datainfo[outlet][\"num_tweets\"]\n",
        "# data_tbl = data_tbl +'|---------------|-----------------------|-----------------------|---------------|\\n' + '|\\t\\t|\\t\\t\\t|\\t\\t\\t|' + str(total_tweets) + '\\t\\t|'\n",
        "# data_tbl = data_tbl +'\\n=================================================================================\\n'\n",
        "# print(data_tbl)\n",
        "\n",
        "# news_df.set_index('timestamp', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resampled_df = news_df.resample('4H').agg({\n",
        "#     'bbc': list,\n",
        "#     'cnn': list,\n",
        "#     'eco': list,\n",
        "#     'tweet': list,\n",
        "#     'replies_count': list,\n",
        "#     'retweets_count': list,\n",
        "#     'likes_count': list\n",
        "# })\n",
        "# # Remove rows where all list columns are empty\n",
        "# resampled_df = resampled_df[~resampled_df.apply(lambda row: all(len(x) == 0 for x in row), axis=1)]"
      ],
      "metadata": {
        "id": "8AMv7KT7S3tb"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df[['positive', 'negative', 'neutral']] = 0\n",
        "news_tweets = news_df[news_df['eco']==0]"
      ],
      "metadata": {
        "id": "9pdZnN3MPdTG"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
      ],
      "metadata": {
        "id": "01CP2YTRpoBk"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in news_tweets['tweet']:\n",
        "  encoded_input = tokenizer(text, padding=True, return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = model(**encoded_input)\n",
        "  logits = output.logits\n",
        "\n",
        "  probabilities = softmax(logits, dim=1)\n",
        "  predicted_class = torch.argmax(probabilities).item()\n",
        "  # Mapping indices to classes based on the usual setup for finbert\n",
        "  news_tweets.iloc[1000, news_tweets.columns.get_indexer(['positive', 'negative', 'neutral'])] = probabilities[0].tolist()\n"
      ],
      "metadata": {
        "id": "1QKcvjPsrtoA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "2ea18f2e-975f-4efc-c6aa-3f63b9612319"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'news_tweets' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-686b53bacc84>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnews_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'news_tweets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "EnxLy08RPjhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities[0].tolist()"
      ],
      "metadata": {
        "id": "ZV76rbP-8L8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HEHyS9Oh5qLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_tweets.iloc[1000]['positive', 'negative', 'neutral']\n",
        "\n",
        "\n",
        "print(news_tweets.iloc[1000])\n",
        "print(\"=========\")\n",
        "print(list(probabilities[0]))"
      ],
      "metadata": {
        "id": "AVW2d5_QP38M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_df['tweet'][101][1]\n",
        "\n",
        "# Tokenize the tweets in the block\n",
        "encoded_input = tokenizer(resampled_df['tweet'][101][1], padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Make sure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Running the model and getting the logits\n",
        "with tf.device('/cpu:0'):  # Assuming you're using CPU; change to '/gpu:0' if using GPU\n",
        "    outputs = model(**encoded_input)\n",
        "    logits = outputs.logits"
      ],
      "metadata": {
        "id": "00TD-_9Louui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resolution = 4*60 # the blocks of time used for predition in min (eg: every 1 hour, every 4 hours, daily, weekly, every 30 min...)\n"
      ],
      "metadata": {
        "id": "ubXgMKibHY02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JZcEccq2puJ"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/Colab Notebooks/datasets/forex_data/DAT_MT_GBPUSD_M1' # location of the forex data\n",
        "\n",
        "forex_data = pd.DataFrame()\n",
        "column_names = ['date','time','open','high','low','close','na']\n",
        "\n",
        "for f in os.listdir(data_path):\n",
        "  data = pd.read_csv(data_path + '/' + f, names=column_names)\n",
        "\n",
        "  # Formatting data and creating timestamps\n",
        "  data['date'] = data['date'].str.replace('.', '-')\n",
        "  data['timestamp'] = pd.to_datetime((data['date'] + ' ' + data['time']))\n",
        "\n",
        "  forex_data = pd.concat([forex_data,data])\n",
        "\n",
        "# Removing duplicates and sorting by time.\n",
        "forex_data = forex_data[['timestamp','open','high','low','close']].drop_duplicates().sort_values(by='timestamp')\n",
        "\n",
        "# Adding in missing timestamps and interpolating the forex prices between those values.\n",
        "forex_data = forex_data.set_index('timestamp')[['open','high','low','close']].asfreq(freq='60s').interpolate()\n",
        "\n",
        "# Smoothing out closing data over 4H to remove noise using Exponential Moving Average and Simple Moving Average\n",
        "period = 60*4\n",
        "forex_data['4hemw'] = forex_data['close'].ewm(span=period, adjust=False).mean() # Exponential Moving Average\n",
        "forex_data['ma'] = forex_data['close'].rolling(window=period).mean() # Simple Moving Average\n",
        "forex_data['ma'] = forex_data['ma'].shift(-int(np.round(period/2))) # Smoothing out stock prices\n",
        "\n",
        "# Calculating the rate of change of the average\n",
        "forex_data['pert_change'] = np.gradient(forex_data['ma'])\n",
        "forex_data['pert_change'] = forex_data['pert_change'].rolling(window=period).mean() # can try ema here as well\n",
        "forex_data['pert_change'] = forex_data['pert_change'].shift(-int(np.round(period/2)))\n",
        "\n",
        "forex_data['log_change']  = np.log(1 + forex_data['pert_change']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2uzqPG4uglK"
      },
      "source": [
        "# Loading BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yc3GQlNKjS-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2mdiQwl8nXr"
      },
      "outputs": [],
      "source": [
        "sentiments = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygpkn9jZDxQv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIJ-sWFy12GD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "undefined.undefined.undefined"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}